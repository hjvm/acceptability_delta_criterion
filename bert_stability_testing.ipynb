{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from bert_training import training\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from transformers import FillMaskPipeline, TextClassificationPipeline, pipeline\n",
    "sns.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random seeds to shuffle the trianing data.\n",
    "RANDOM_SEEDS = np.random.choice(1000, 200, replace=False).tolist()\n",
    "\n",
    "## BERT models.\n",
    "BERT = 'bert-base-uncased'\n",
    "\n",
    "## Data of interest.\n",
    "TRAIN_DIR = Path('./data/CoLA/cola_in_domain_train.csv')\n",
    "TEST_DIR = Path('./data/li-adger_sentences.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the .csv file with human data into memory.\n",
    "train_data = pd.read_csv(TRAIN_DIR, index_col=0)\n",
    "test_data = pd.read_csv(TEST_DIR)[['id', 'sentence', 'dataset']]\n",
    "\n",
    "# Create new dataframe only with unique sentences and human judgements.\n",
    "test_data = test_data.drop_duplicates(subset=['id'], keep='first').rename(columns={'regularizedSentence':'sentence'}).reset_index(drop=True)\n",
    "\n",
    "# Get expert labels for each sentence.\n",
    "test_data['label'] = (test_data.id.str.contains('g')).astype('int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "## Set up CUDA for training.\n",
    "if torch.cuda.is_available():  # Tell PyTorch to use the GPU. \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('Will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function to apply Sequence Classification pipeline to a Pandas DataFrame.\n",
    "def sentence_acc_score(dataframe, sent_col='sentence', acc_pipeline=None):\n",
    "    sentences = dataframe[sent_col].values.tolist()\n",
    "    \n",
    "    try: # Check whether all the sentences can be processed simultaneously.\n",
    "        results = pd.DataFrame(acc_pipeline(sentences))\n",
    "    except: # Batch them if not.\n",
    "        n = 500 # Batch size.\n",
    "        results = []\n",
    "        for i in range(0, len(sentences), n):\n",
    "            batch = sentences[i:i+n] if i+n < len(sentences) else sentences[i:]\n",
    "            results += acc_pipeline(batch)\n",
    "            torch.cuda.empty_cache()\n",
    "        results = pd.DataFrame(results)\n",
    "    \n",
    "    results = pd.DataFrame(acc_pipeline(sentences))\n",
    "    results.label.replace({'LABEL_0':0, 'LABEL_1':1}, inplace=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_pred, y_true):\n",
    "    matches = y_true.compare(y_pred, keep_shape=True, keep_equal=True)\n",
    "    \n",
    "    false_positives = matches.loc[np.logical_and(y_true==0, y_pred==1.0)].shape[0]\n",
    "    true_positives = y_pred.loc[np.logical_and(y_true==1.0, y_pred==1.0)].shape[0]\n",
    "    \n",
    "    precision = true_positives/(true_positives + false_positives)\n",
    "    recall = true_positives/y_true.loc[y_true == 1].shape[0]\n",
    "    \n",
    "    return 2 * (precision * recall)/(precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## Load first pretrained BERT.\n",
    "bert = BertForSequenceClassification.from_pretrained(BERT)\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT)\n",
    "\n",
    "# Initialize a Sequence Classification pipeline for testing.\n",
    "bert_pipeline = pipeline(task='sentiment-analysis', model=bert, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Apply the pipeline to the test data before training.\n",
    "baseline = sentence_acc_score(test_data, sent_col='sentence', acc_pipeline=bert_pipeline)\n",
    "test_data['baseline'] = baseline.label.values\n",
    "\n",
    "# Clear GPU memory (NOTE: using torch.cuda.empty_cache() is bad practice...).\n",
    "#bert.to(\"cpu\")\n",
    "#bert_pipeline = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:01.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 518 performance:\n",
      "F1 score: 0.9695561236346735\n",
      "MCC score: 0.5253572611643103\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 702 performance:\n",
      "F1 score: 0.9645025688930406\n",
      "MCC score: 0.5346573760704004\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 898 performance:\n",
      "F1 score: 0.9625643425362658\n",
      "MCC score: 0.5406158543129779\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 658 performance:\n",
      "F1 score: 0.9596433599249178\n",
      "MCC score: 0.5429792915885718\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 341 performance:\n",
      "F1 score: 0.9645025688930406\n",
      "MCC score: 0.5480722127146332\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 810 performance:\n",
      "F1 score: 0.9632920271218143\n",
      "MCC score: 0.5120220018338578\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 8 performance:\n",
      "F1 score: 0.9637765833138584\n",
      "MCC score: 0.5319475573540418\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.79\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 559 performance:\n",
      "F1 score: 0.9630495790458372\n",
      "MCC score: 0.534016674950612\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 457 performance:\n",
      "F1 score: 0.9593992020652429\n",
      "MCC score: 0.5214083928348404\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 363 performance:\n",
      "F1 score: 0.9625643425362658\n",
      "MCC score: 0.5314924652051738\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.20\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 978 performance:\n",
      "F1 score: 0.9661925856843088\n",
      "MCC score: 0.5475986956094736\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 106 performance:\n",
      "F1 score: 0.9657102869139258\n",
      "MCC score: 0.5450775460804709\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 431 performance:\n",
      "F1 score: 0.9611059044048735\n",
      "MCC score: 0.5422126628431743\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.10\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 163 performance:\n",
      "F1 score: 0.9649859943977591\n",
      "MCC score: 0.5445361228652948\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 743 performance:\n",
      "F1 score: 0.9659514925373135\n",
      "MCC score: 0.5469852793576683\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 123 performance:\n",
      "F1 score: 0.9645025688930406\n",
      "MCC score: 0.5498019153556679\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 665 performance:\n",
      "F1 score: 0.964260686755431\n",
      "MCC score: 0.5258055427354921\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 180 performance:\n",
      "F1 score: 0.9645025688930406\n",
      "MCC score: 0.5303253987534388\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 227 performance:\n",
      "F1 score: 0.9659514925373135\n",
      "MCC score: 0.5327394046323312\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 39 performance:\n",
      "F1 score: 0.9669151910531221\n",
      "MCC score: 0.5421209106061643\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 677 performance:\n",
      "F1 score: 0.9628070175438596\n",
      "MCC score: 0.551200594559527\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 894 performance:\n",
      "F1 score: 0.9630495790458372\n",
      "MCC score: 0.5522358694002042\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 439 performance:\n",
      "F1 score: 0.964260686755431\n",
      "MCC score: 0.5431368036036056\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 688 performance:\n",
      "F1 score: 0.9647443380807845\n",
      "MCC score: 0.5287766724066552\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 997 performance:\n",
      "F1 score: 0.9666744348636681\n",
      "MCC score: 0.527264241392592\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 187 performance:\n",
      "F1 score: 0.9664335664335664\n",
      "MCC score: 0.5240451623381912\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 725 performance:\n",
      "F1 score: 0.9620786516853933\n",
      "MCC score: 0.5537405005044721\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 602 performance:\n",
      "F1 score: 0.9620786516853933\n",
      "MCC score: 0.5337527551808738\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 474 performance:\n",
      "F1 score: 0.9632920271218143\n",
      "MCC score: 0.5437363187289816\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 501 performance:\n",
      "F1 score: 0.9611059044048735\n",
      "MCC score: 0.5404714194995401\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.79\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 391 performance:\n",
      "F1 score: 0.9620786516853933\n",
      "MCC score: 0.5554773492003833\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 496 performance:\n",
      "F1 score: 0.9652275379229872\n",
      "MCC score: 0.5390979074654113\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 857 performance:\n",
      "F1 score: 0.9669151910531221\n",
      "MCC score: 0.5477196355171197\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 205 performance:\n",
      "F1 score: 0.964260686755431\n",
      "MCC score: 0.53404280751596\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 869 performance:\n",
      "F1 score: 0.964260686755431\n",
      "MCC score: 0.5240700300110691\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 471 performance:\n",
      "F1 score: 0.9649859943977591\n",
      "MCC score: 0.5704470685666403\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 438 performance:\n",
      "F1 score: 0.9635343618513323\n",
      "MCC score: 0.5443431341083385\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 517 performance:\n",
      "F1 score: 0.9613492621222769\n",
      "MCC score: 0.5358504399482489\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 400 performance:\n",
      "F1 score: 0.9673963670237541\n",
      "MCC score: 0.5373319654959128\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.79\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 546 performance:\n",
      "F1 score: 0.9649859943977591\n",
      "MCC score: 0.5298284527912531\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.87\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 494 performance:\n",
      "F1 score: 0.9632920271218143\n",
      "MCC score: 0.547204279576866\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.77\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.10\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 215 performance:\n",
      "F1 score: 0.9676367869615833\n",
      "MCC score: 0.5362329046538689\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.79\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 131 performance:\n",
      "F1 score: 0.9697955390334572\n",
      "MCC score: 0.524704952626748\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 629 performance:\n",
      "F1 score: 0.9654689687354177\n",
      "MCC score: 0.5448962076346685\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 761 performance:\n",
      "F1 score: 0.9678770949720671\n",
      "MCC score: 0.5321216613859154\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 687 performance:\n",
      "F1 score: 0.961592505854801\n",
      "MCC score: 0.5534140666271311\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:16.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.77\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 154 performance:\n",
      "F1 score: 0.9645025688930406\n",
      "MCC score: 0.5307587168576349\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.79\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 288 performance:\n",
      "F1 score: 0.9647443380807845\n",
      "MCC score: 0.5521380747054604\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 979 performance:\n",
      "F1 score: 0.9657102869139258\n",
      "MCC score: 0.5338482667844386\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.79\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 108 performance:\n",
      "F1 score: 0.9664335664335664\n",
      "MCC score: 0.534412847377642\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 38 performance:\n",
      "F1 score: 0.9628070175438596\n",
      "MCC score: 0.5151370177872724\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 568 performance:\n",
      "F1 score: 0.9645025688930406\n",
      "MCC score: 0.5510990469149172\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 906 performance:\n",
      "F1 score: 0.9661925856843088\n",
      "MCC score: 0.5385406663268109\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.78\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 767 performance:\n",
      "F1 score: 0.9613492621222769\n",
      "MCC score: 0.5201626546206634\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 555 performance:\n",
      "F1 score: 0.961592505854801\n",
      "MCC score: 0.531232022777586\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.10\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 331 performance:\n",
      "F1 score: 0.9596433599249178\n",
      "MCC score: 0.5434156438006583\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 593 performance:\n",
      "F1 score: 0.9625643425362658\n",
      "MCC score: 0.5458255100322271\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.10\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 776 performance:\n",
      "F1 score: 0.9657102869139258\n",
      "MCC score: 0.526063822122123\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 167 performance:\n",
      "F1 score: 0.9666744348636681\n",
      "MCC score: 0.5505495203517654\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.79\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 251 performance:\n",
      "F1 score: 0.9649859943977591\n",
      "MCC score: 0.5220292818839922\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 117 performance:\n",
      "F1 score: 0.9630495790458372\n",
      "MCC score: 0.5444311689183355\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 408 performance:\n",
      "F1 score: 0.9659514925373135\n",
      "MCC score: 0.543965410177673\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.79\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 508 performance:\n",
      "F1 score: 0.9603751465416178\n",
      "MCC score: 0.5543544472281091\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 404 performance:\n",
      "F1 score: 0.9601313320825515\n",
      "MCC score: 0.5467883335906304\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.79\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 440 performance:\n",
      "F1 score: 0.9628070175438596\n",
      "MCC score: 0.5572715456732594\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 928 performance:\n",
      "F1 score: 0.9623215539433653\n",
      "MCC score: 0.5395756995076433\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 987 performance:\n",
      "F1 score: 0.9659514925373135\n",
      "MCC score: 0.5525913986446407\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 515 performance:\n",
      "F1 score: 0.9671558350803634\n",
      "MCC score: 0.5311049370576455\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 854 performance:\n",
      "F1 score: 0.9591549295774647\n",
      "MCC score: 0.5479009610614257\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 967 performance:\n",
      "F1 score: 0.9608624326224514\n",
      "MCC score: 0.5485780721525652\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 975 performance:\n",
      "F1 score: 0.9640186915887851\n",
      "MCC score: 0.5321280806678027\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 82 performance:\n",
      "F1 score: 0.9608624326224514\n",
      "MCC score: 0.5611984415296383\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 500 performance:\n",
      "F1 score: 0.9652275379229872\n",
      "MCC score: 0.5490349970564338\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 86 performance:\n",
      "F1 score: 0.9681172911333489\n",
      "MCC score: 0.541782289983702\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 838 performance:\n",
      "F1 score: 0.9654689687354177\n",
      "MCC score: 0.538416723171354\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 53 performance:\n",
      "F1 score: 0.961592505854801\n",
      "MCC score: 0.5373254827188704\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 580 performance:\n",
      "F1 score: 0.9628070175438596\n",
      "MCC score: 0.5286264586027463\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 610 performance:\n",
      "F1 score: 0.9666744348636681\n",
      "MCC score: 0.548396187389236\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.79\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 44 performance:\n",
      "F1 score: 0.9623215539433653\n",
      "MCC score: 0.5443537673926766\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 796 performance:\n",
      "F1 score: 0.961592505854801\n",
      "MCC score: 0.5316674110869334\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 3 performance:\n",
      "F1 score: 0.9649859943977591\n",
      "MCC score: 0.5298284527912531\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 533 performance:\n",
      "F1 score: 0.9630495790458372\n",
      "MCC score: 0.5301080243389015\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 225 performance:\n",
      "F1 score: 0.9601313320825515\n",
      "MCC score: 0.5214794251116628\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 284 performance:\n",
      "F1 score: 0.9683573755234993\n",
      "MCC score: 0.5359578271054604\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 878 performance:\n",
      "F1 score: 0.9640186915887851\n",
      "MCC score: 0.5373292250548927\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 694 performance:\n",
      "F1 score: 0.9632920271218143\n",
      "MCC score: 0.5359293015191606\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 801 performance:\n",
      "F1 score: 0.9623215539433653\n",
      "MCC score: 0.5334912789560904\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 900 performance:\n",
      "F1 score: 0.9640186915887851\n",
      "MCC score: 0.537762502532234\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 490 performance:\n",
      "F1 score: 0.9576868829337095\n",
      "MCC score: 0.5351557584376073\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 868 performance:\n",
      "F1 score: 0.9640186915887851\n",
      "MCC score: 0.5381957582302617\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 912 performance:\n",
      "F1 score: 0.9630495790458372\n",
      "MCC score: 0.5279356523321018\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 830 performance:\n",
      "F1 score: 0.9688372093023256\n",
      "MCC score: 0.5483729111044021\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 926 performance:\n",
      "F1 score: 0.9611059044048735\n",
      "MCC score: 0.5483052893458874\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 451 performance:\n",
      "F1 score: 0.9647443380807845\n",
      "MCC score: 0.5430600704642167\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 126 performance:\n",
      "F1 score: 0.9647443380807845\n",
      "MCC score: 0.5365708494936262\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 681 performance:\n",
      "F1 score: 0.9623215539433653\n",
      "MCC score: 0.538706735280549\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:39.\n",
      " Batch   240 of   385. Elapsed: 0:00:47.\n",
      " Batch   280 of   385. Elapsed: 0:00:55.\n",
      " Batch   320 of   385. Elapsed: 0:01:03.\n",
      " Batch   360 of   385. Elapsed: 0:01:10.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:15\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:54.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:09.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:13\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 93 performance:\n",
      "F1 score: 0.96183563568251\n",
      "MCC score: 0.5309693171278731\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 102 performance:\n",
      "F1 score: 0.9647443380807845\n",
      "MCC score: 0.5417626000501687\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 735 performance:\n",
      "F1 score: 0.9700348432055749\n",
      "MCC score: 0.5365004065848425\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:08.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:08.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:13\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 121 performance:\n",
      "F1 score: 0.9652275379229872\n",
      "MCC score: 0.5369362348681883\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:16.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 950 performance:\n",
      "F1 score: 0.9676367869615833\n",
      "MCC score: 0.5418283021341119\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 148 performance:\n",
      "F1 score: 0.9688372093023256\n",
      "MCC score: 0.521726297673407\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 143 performance:\n",
      "F1 score: 0.9571966133584195\n",
      "MCC score: 0.5602571392900512\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 836 performance:\n",
      "F1 score: 0.9625643425362658\n",
      "MCC score: 0.5358380992748142\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 982 performance:\n",
      "F1 score: 0.9640186915887851\n",
      "MCC score: 0.5260553181095633\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.31\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.20\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.14\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 31 performance:\n",
      "F1 score: 0.9657102869139258\n",
      "MCC score: 0.5407603885255037\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 220 performance:\n",
      "F1 score: 0.9647443380807845\n",
      "MCC score: 0.5274767669770699\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 839 performance:\n",
      "F1 score: 0.9652275379229872\n",
      "MCC score: 0.5438516717731685\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 806 performance:\n",
      "F1 score: 0.9652275379229872\n",
      "MCC score: 0.5356389460090115\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 78 performance:\n",
      "F1 score: 0.9630495790458372\n",
      "MCC score: 0.5405270244198788\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 47 performance:\n",
      "F1 score: 0.9635343618513323\n",
      "MCC score: 0.540008787462695\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 970 performance:\n",
      "F1 score: 0.9647443380807845\n",
      "MCC score: 0.5296431283168482\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 936 performance:\n",
      "F1 score: 0.9632920271218143\n",
      "MCC score: 0.5393998636874558\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 378 performance:\n",
      "F1 score: 0.9657102869139258\n",
      "MCC score: 0.5321192108263707\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.78\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:16.\n",
      " Batch   120 of   385. Elapsed: 0:00:25.\n",
      " Batch   160 of   385. Elapsed: 0:00:32.\n",
      " Batch   200 of   385. Elapsed: 0:00:39.\n",
      " Batch   240 of   385. Elapsed: 0:00:47.\n",
      " Batch   280 of   385. Elapsed: 0:00:54.\n",
      " Batch   320 of   385. Elapsed: 0:01:02.\n",
      " Batch   360 of   385. Elapsed: 0:01:10.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:14\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:16.\n",
      " Batch   120 of   385. Elapsed: 0:00:24.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:54.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:09.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:14\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 723 performance:\n",
      "F1 score: 0.9625643425362658\n",
      "MCC score: 0.5453914567726238\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:54.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:09.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:14\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:08.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:13\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:54.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:09.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:13\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:54.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:09.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:14\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 918 performance:\n",
      "F1 score: 0.9664335664335664\n",
      "MCC score: 0.5563970815449303\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:16.\n",
      " Batch   120 of   385. Elapsed: 0:00:24.\n",
      " Batch   160 of   385. Elapsed: 0:00:32.\n",
      " Batch   200 of   385. Elapsed: 0:00:39.\n",
      " Batch   240 of   385. Elapsed: 0:00:47.\n",
      " Batch   280 of   385. Elapsed: 0:00:55.\n",
      " Batch   320 of   385. Elapsed: 0:01:02.\n",
      " Batch   360 of   385. Elapsed: 0:01:10.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:15\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:08.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:13\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:08.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:13\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:08.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 679 performance:\n",
      "F1 score: 0.964260686755431\n",
      "MCC score: 0.549627430624923\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:08.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:08.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:08.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:13\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:09.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:14\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 206 performance:\n",
      "F1 score: 0.9654689687354177\n",
      "MCC score: 0.5431687956128161\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:16.\n",
      " Batch   120 of   385. Elapsed: 0:00:25.\n",
      " Batch   160 of   385. Elapsed: 0:00:33.\n",
      " Batch   200 of   385. Elapsed: 0:00:40.\n",
      " Batch   240 of   385. Elapsed: 0:00:48.\n",
      " Batch   280 of   385. Elapsed: 0:00:56.\n",
      " Batch   320 of   385. Elapsed: 0:01:04.\n",
      " Batch   360 of   385. Elapsed: 0:01:12.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:17\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:16.\n",
      " Batch   120 of   385. Elapsed: 0:00:24.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:39.\n",
      " Batch   240 of   385. Elapsed: 0:00:47.\n",
      " Batch   280 of   385. Elapsed: 0:00:55.\n",
      " Batch   320 of   385. Elapsed: 0:01:02.\n",
      " Batch   360 of   385. Elapsed: 0:01:10.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:14\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:54.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:09.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:14\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:16.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:39.\n",
      " Batch   240 of   385. Elapsed: 0:00:47.\n",
      " Batch   280 of   385. Elapsed: 0:00:55.\n",
      " Batch   320 of   385. Elapsed: 0:01:03.\n",
      " Batch   360 of   385. Elapsed: 0:01:11.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:16\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 248 performance:\n",
      "F1 score: 0.9623215539433653\n",
      "MCC score: 0.5330565180105533\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:09.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:13\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:08.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:13\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 930 performance:\n",
      "F1 score: 0.9652275379229872\n",
      "MCC score: 0.5334762946848641\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:39.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:54.\n",
      " Batch   320 of   385. Elapsed: 0:01:02.\n",
      " Batch   360 of   385. Elapsed: 0:01:09.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:13\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 315 performance:\n",
      "F1 score: 0.9649859943977591\n",
      "MCC score: 0.5363205090035036\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 554 performance:\n",
      "F1 score: 0.9661925856843088\n",
      "MCC score: 0.5316323380944994\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:16.\n",
      " Batch   120 of   385. Elapsed: 0:00:24.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:39.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:54.\n",
      " Batch   320 of   385. Elapsed: 0:01:02.\n",
      " Batch   360 of   385. Elapsed: 0:01:09.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:14\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 244 performance:\n",
      "F1 score: 0.9664335664335664\n",
      "MCC score: 0.5214502373717397\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:16.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:08.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:13\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 270 performance:\n",
      "F1 score: 0.9673963670237541\n",
      "MCC score: 0.5356089363795312\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:09.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:14\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:08.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:13\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 482 performance:\n",
      "F1 score: 0.9657102869139258\n",
      "MCC score: 0.5433509246627994\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:11\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:39.\n",
      " Batch   240 of   385. Elapsed: 0:00:46.\n",
      " Batch   280 of   385. Elapsed: 0:00:54.\n",
      " Batch   320 of   385. Elapsed: 0:01:01.\n",
      " Batch   360 of   385. Elapsed: 0:01:09.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:14\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 119 performance:\n",
      "F1 score: 0.9623215539433653\n",
      "MCC score: 0.5230500275312122\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.10\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 541 performance:\n",
      "F1 score: 0.9620786516853933\n",
      "MCC score: 0.5493977840555746\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 653 performance:\n",
      "F1 score: 0.96183563568251\n",
      "MCC score: 0.5335803295158911\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 963 performance:\n",
      "F1 score: 0.9640186915887851\n",
      "MCC score: 0.5334286952197183\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 325 performance:\n",
      "F1 score: 0.9596433599249178\n",
      "MCC score: 0.5342492643330029\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 249 performance:\n",
      "F1 score: 0.9649859943977591\n",
      "MCC score: 0.5354552387602994\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 386 performance:\n",
      "F1 score: 0.9669151910531221\n",
      "MCC score: 0.5416900916030197\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 116 performance:\n",
      "F1 score: 0.9647443380807845\n",
      "MCC score: 0.5443573709506112\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 764 performance:\n",
      "F1 score: 0.9661925856843088\n",
      "MCC score: 0.5329281796299286\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 149 performance:\n",
      "F1 score: 0.9576868829337095\n",
      "MCC score: 0.5285832535063338\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:31.\n",
      " Batch   200 of   385. Elapsed: 0:00:38.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:53.\n",
      " Batch   320 of   385. Elapsed: 0:01:00.\n",
      " Batch   360 of   385. Elapsed: 0:01:07.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:12\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 257 performance:\n",
      "F1 score: 0.9681172911333489\n",
      "MCC score: 0.5430718059987465\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 567 performance:\n",
      "F1 score: 0.9664335664335664\n",
      "MCC score: 0.5451957499718082\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 575 performance:\n",
      "F1 score: 0.9613492621222769\n",
      "MCC score: 0.5367211007544144\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 441 performance:\n",
      "F1 score: 0.9628070175438596\n",
      "MCC score: 0.5260179206473402\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 615 performance:\n",
      "F1 score: 0.9630495790458372\n",
      "MCC score: 0.5461659228933149\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 763 performance:\n",
      "F1 score: 0.960618846694796\n",
      "MCC score: 0.5397046426436541\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 993 performance:\n",
      "F1 score: 0.9623215539433653\n",
      "MCC score: 0.5378376956877154\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.79\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 582 performance:\n",
      "F1 score: 0.9640186915887851\n",
      "MCC score: 0.5334286952197183\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.77\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:01.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 409 performance:\n",
      "F1 score: 0.9628070175438596\n",
      "MCC score: 0.5429579646592935\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.79\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 753 performance:\n",
      "F1 score: 0.9611059044048735\n",
      "MCC score: 0.5356816234369345\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 654 performance:\n",
      "F1 score: 0.9666744348636681\n",
      "MCC score: 0.5246719469841123\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:06.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 614 performance:\n",
      "F1 score: 0.960618846694796\n",
      "MCC score: 0.5253148986547059\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 910 performance:\n",
      "F1 score: 0.9640186915887851\n",
      "MCC score: 0.545990995507059\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 676 performance:\n",
      "F1 score: 0.9567058823529412\n",
      "MCC score: 0.530148003683083\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 208 performance:\n",
      "F1 score: 0.96183563568251\n",
      "MCC score: 0.5283574465416437\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 247 performance:\n",
      "F1 score: 0.9601313320825515\n",
      "MCC score: 0.5541972424537829\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 447 performance:\n",
      "F1 score: 0.9564603436102612\n",
      "MCC score: 0.5387723272428273\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 85 performance:\n",
      "F1 score: 0.9654689687354177\n",
      "MCC score: 0.525873947031319\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 91 performance:\n",
      "F1 score: 0.9603751465416178\n",
      "MCC score: 0.5351801023820965\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 416 performance:\n",
      "F1 score: 0.9647443380807845\n",
      "MCC score: 0.5331078268348778\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 959 performance:\n",
      "F1 score: 0.9640186915887851\n",
      "MCC score: 0.5576747843946556\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 128 performance:\n",
      "F1 score: 0.961592505854801\n",
      "MCC score: 0.5355849094691876\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 495 performance:\n",
      "F1 score: 0.9666744348636681\n",
      "MCC score: 0.5363283228869565\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 771 performance:\n",
      "F1 score: 0.9637765833138584\n",
      "MCC score: 0.5380179042095328\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 353 performance:\n",
      "F1 score: 0.9659514925373135\n",
      "MCC score: 0.5318751516616522\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.31\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.20\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.14\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 778 performance:\n",
      "F1 score: 0.9625643425362658\n",
      "MCC score: 0.526274449222267\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:44.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 785 performance:\n",
      "F1 score: 0.9671558350803634\n",
      "MCC score: 0.5371402915030337\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 946 performance:\n",
      "F1 score: 0.9608624326224514\n",
      "MCC score: 0.544224658843649\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 297 performance:\n",
      "F1 score: 0.960618846694796\n",
      "MCC score: 0.5379615763102442\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 716 performance:\n",
      "F1 score: 0.9659514925373135\n",
      "MCC score: 0.5422393421242108\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 775 performance:\n",
      "F1 score: 0.9593992020652429\n",
      "MCC score: 0.5489303133417294\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 87 performance:\n",
      "F1 score: 0.9632920271218143\n",
      "MCC score: 0.5476377083101975\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 526 performance:\n",
      "F1 score: 0.9673963670237541\n",
      "MCC score: 0.540346256637233\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 915 performance:\n",
      "F1 score: 0.9654689687354177\n",
      "MCC score: 0.5392809329207556\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:45.\n",
      " Batch   280 of   385. Elapsed: 0:00:52.\n",
      " Batch   320 of   385. Elapsed: 0:00:59.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:10\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:23.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Random seed 263 performance:\n",
      "F1 score: 0.9654689687354177\n",
      "MCC score: 0.5401450523516778\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:50.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:58.\n",
      " Batch   360 of   385. Elapsed: 0:01:05.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:02\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:30.\n",
      " Batch   200 of   385. Elapsed: 0:00:37.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:51.\n",
      " Batch   320 of   385. Elapsed: 0:00:57.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:09\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 228 performance:\n",
      "F1 score: 0.964260686755431\n",
      "MCC score: 0.528407887700336\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 170 performance:\n",
      "F1 score: 0.9586660403945514\n",
      "MCC score: 0.5423457432869854\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 532 performance:\n",
      "F1 score: 0.9603751465416178\n",
      "MCC score: 0.5547900111756905\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.19\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:01.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:05\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 534 performance:\n",
      "F1 score: 0.9635343618513323\n",
      "MCC score: 0.5495422142924127\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 253 performance:\n",
      "F1 score: 0.9664335664335664\n",
      "MCC score: 0.5426091515070549\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.86\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 383 performance:\n",
      "F1 score: 0.9652275379229872\n",
      "MCC score: 0.5360714001486436\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.50\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:01.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 544 performance:\n",
      "F1 score: 0.96183563568251\n",
      "MCC score: 0.5409741394755597\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 893 performance:\n",
      "F1 score: 0.964260686755431\n",
      "MCC score: 0.5271068503778665\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:47.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 360 performance:\n",
      "F1 score: 0.961592505854801\n",
      "MCC score: 0.5381956548704304\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 795 performance:\n",
      "F1 score: 0.9695561236346735\n",
      "MCC score: 0.5463852881968447\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 55 performance:\n",
      "F1 score: 0.9569513055751588\n",
      "MCC score: 0.5140551082478095\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 608 performance:\n",
      "F1 score: 0.9647443380807845\n",
      "MCC score: 0.5352724019195784\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 814 performance:\n",
      "F1 score: 0.9673963670237541\n",
      "MCC score: 0.5437897084538257\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:47.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 953 performance:\n",
      "F1 score: 0.9664335664335664\n",
      "MCC score: 0.5387277879039064\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 60 performance:\n",
      "F1 score: 0.9647443380807845\n",
      "MCC score: 0.5322418157743931\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.30\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:01.\n",
      "\n",
      " Average training loss: 0.13\n",
      " Training epoch took: 0:01:05\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 337 performance:\n",
      "F1 score: 0.964260686755431\n",
      "MCC score: 0.5379414038363961\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:47.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 492 performance:\n",
      "F1 score: 0.960618846694796\n",
      "MCC score: 0.5357823560578044\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:54.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 306 performance:\n",
      "F1 score: 0.964260686755431\n",
      "MCC score: 0.5431368036036056\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:01.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:05\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 904 performance:\n",
      "F1 score: 0.9647443380807845\n",
      "MCC score: 0.545222147697546\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:54.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:01.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 282 performance:\n",
      "F1 score: 0.9608624326224514\n",
      "MCC score: 0.5507543879078001\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 669 performance:\n",
      "F1 score: 0.9635343618513323\n",
      "MCC score: 0.5469429357527442\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:01.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:05\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 628 performance:\n",
      "F1 score: 0.9603751465416178\n",
      "MCC score: 0.5207768771486653\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 826 performance:\n",
      "F1 score: 0.9669151910531221\n",
      "MCC score: 0.5300483530256498\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:01.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 512 performance:\n",
      "F1 score: 0.9608624326224514\n",
      "MCC score: 0.5498838889243125\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:04.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:08\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:01.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:05\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 392 performance:\n",
      "F1 score: 0.9647443380807845\n",
      "MCC score: 0.541330070689999\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.47\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:43.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.27\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.16\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:47.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 176 performance:\n",
      "F1 score: 0.9589105423808406\n",
      "MCC score: 0.5403181867895142\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.80\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 661 performance:\n",
      "F1 score: 0.964260686755431\n",
      "MCC score: 0.5461662086540353\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.49\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.79\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:08.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.29\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:26.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 324 performance:\n",
      "F1 score: 0.96183563568251\n",
      "MCC score: 0.5457557211769645\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.82\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:15.\n",
      " Batch   120 of   385. Elapsed: 0:00:22.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.84\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.17\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.83\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:40.\n",
      " Batch   280 of   385. Elapsed: 0:00:47.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.11\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.85\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 77 performance:\n",
      "F1 score: 0.9574418057841524\n",
      "MCC score: 0.5240386864971813\n",
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n",
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:33.\n",
      " Batch   240 of   385. Elapsed: 0:00:41.\n",
      " Batch   280 of   385. Elapsed: 0:00:48.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:03.\n",
      "\n",
      " Average training loss: 0.48\n",
      " Training epoch took: 0:01:07\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:27.\n",
      " Batch   200 of   385. Elapsed: 0:00:34.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:56.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.28\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:06.\n",
      " Batch    80 of   385. Elapsed: 0:00:13.\n",
      " Batch   120 of   385. Elapsed: 0:00:20.\n",
      " Batch   160 of   385. Elapsed: 0:00:28.\n",
      " Batch   200 of   385. Elapsed: 0:00:35.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.18\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      " Batch    40 of   385. Elapsed: 0:00:07.\n",
      " Batch    80 of   385. Elapsed: 0:00:14.\n",
      " Batch   120 of   385. Elapsed: 0:00:21.\n",
      " Batch   160 of   385. Elapsed: 0:00:29.\n",
      " Batch   200 of   385. Elapsed: 0:00:36.\n",
      " Batch   240 of   385. Elapsed: 0:00:42.\n",
      " Batch   280 of   385. Elapsed: 0:00:49.\n",
      " Batch   320 of   385. Elapsed: 0:00:55.\n",
      " Batch   360 of   385. Elapsed: 0:01:02.\n",
      "\n",
      " Average training loss: 0.12\n",
      " Training epoch took: 0:01:06\n",
      "\n",
      "Running Validation...\n",
      " Accuracy: 0.81\n",
      " Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Random seed 268 performance:\n",
      "F1 score: 0.9666744348636681\n",
      "MCC score: 0.5397780865477149\n"
     ]
    }
   ],
   "source": [
    "seed_performance = []\n",
    "for seed in RANDOM_SEEDS:\n",
    "    # Copy the original model and offload it to the GPU.\n",
    "    bert_cola = deepcopy(bert)\n",
    "    \n",
    "    # Create new pipeline.\n",
    "    bert_cola_pipeline = pipeline(task='sentiment-analysis', model=bert_cola, tokenizer=tokenizer, device=0)\n",
    "  \n",
    "    # Test pipeline to make sure it executes the same predictions as baseline.\n",
    "    new_pred = sentence_acc_score(test_data, sent_col='sentence', acc_pipeline=bert_cola_pipeline)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if not test_data.baseline.compare(new_pred.label).empty:\n",
    "        print(\"Predictions on test set before training changed!\")\n",
    "        print(\"Skipping training for random seed {}...\".format(seed))\n",
    "        continue\n",
    "\n",
    "    # Train the model and recreate the pipeline.\n",
    "    #bert_cola.to(device)\n",
    "    shuffled_data = train_data.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    bert_cola = training(bert_cola, tokenizer, device, shuffled_data, random_seed=seed)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    bert_cola_pipeline = pipeline(task='sentiment-analysis', model=bert_cola, tokenizer=tokenizer, device=0)\n",
    "    \n",
    "    # Get new predictions from test data after fine-tuning with CoLA.\n",
    "    new_pred = sentence_acc_score(test_data, sent_col='sentence', acc_pipeline=bert_cola_pipeline)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    test_data['seed{}'.format(seed)] = new_pred.label.values\n",
    "    \n",
    "    # Get accuracy of the random seed.\n",
    "    f1 = f1_score(test_data['seed{}'.format(seed)], test_data.label)\n",
    "    mcc = matthews_corrcoef(test_data.label, test_data['seed{}'.format(seed)])\n",
    "    seed_performance.append(mcc)\n",
    "    print(\"Random seed {} performance:\\n\".format(seed) +\\\n",
    "          \"F1 score: {}\\n\".format(f1) +\\\n",
    "          \"MCC score: {}\".format(mcc))\n",
    "    \n",
    "    # Clear GPU memory again for next iteration...\n",
    "    bert_cola = None\n",
    "    bert_cola_pipeline = None\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 1272 sentences flipped across 200 random seeds.\n",
      "This is [30.44518909] percent of all the sentences in the test set!\n"
     ]
    }
   ],
   "source": [
    "## Check which predictions flipped according to changes in training order.\n",
    "test_data['flips'] = 0\n",
    "test_data['num_flips'] = 0\n",
    "flip_hist = []\n",
    "for i in range(len(RANDOM_SEEDS)):\n",
    "    seed1 = RANDOM_SEEDS[i]\n",
    "    seed2 = RANDOM_SEEDS[i+1] if i+1 < len(RANDOM_SEEDS) else RANDOM_SEEDS[0]\n",
    "    \n",
    "    # Will return NaN values for rows that did not change.\n",
    "    delta = test_data['seed{}'.format(seed1)].compare(test_data['seed{}'.format(seed2)], keep_shape=True)\n",
    "    \n",
    "    # Merge the two columns.  Rows that changed value will have a new value of 1.  Rows that didn't will still be NaN.\n",
    "    delta = delta.self + delta.other\n",
    "    \n",
    "    # Change 0s to 1s for boolean logic.\n",
    "    delta.replace(to_replace=0, value=1, inplace=True)\n",
    "    \n",
    "    # Replace NaN with 0 for boolean logic.\n",
    "    delta.fillna(0, inplace=True)\n",
    "    \n",
    "    # Perform boolean addition on the current record of flipping sentences.\n",
    "    test_data.flips = test_data.flips | delta.astype('int32')\n",
    "    \n",
    "    # Perform normal addition in order to tally how much each sentence flipped.\n",
    "    test_data.num_flips += delta.astype('int32')\n",
    "    \n",
    "    # Get total of sentences that have flipped so far.\n",
    "    flip_hist.append(test_data.flips.sum()/test_data.flips.shape)    \n",
    "    \n",
    "print(\"A total of {} sentences flipped across {} random seeds.\".format(test_data.flips.sum(), len(RANDOM_SEEDS)))\n",
    "print(\"This is {} percent of all the sentences in the test set!\".format((test_data.flips.sum()/test_data.flips.shape)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9       They appear to that Chris is the right person ...\n",
      "10      She seems to that Garrett should be punished f...\n",
      "11             I appear to that Data is an unsafe driver.\n",
      "12                I seem to that Robert can't be trusted.\n",
      "37                  Charles believed Faith to be unhappy.\n",
      "                              ...                        \n",
      "4134                   Which the train did you arrive on?\n",
      "4135                 Which the bridesmaid did you escort?\n",
      "4137                Which the teacher do you admire most?\n",
      "4146                     I wondered could we leave early.\n",
      "4149              I wondered could we find a replacement.\n",
      "Name: sentence, Length: 1272, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Get all examples of sentences that flip.\n",
    "acrobatic_sentence = test_data.loc[test_data.flips == 1]\n",
    "acrobatic_sentence = acrobatic_sentence[['id', 'sentence', 'dataset', 'num_flips']]\n",
    "print(acrobatic_sentence.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get f1 score that always predicts majority class.\n",
    "majority_label = test_data.label.mode()[0]\n",
    "majority_pred = pd.Series([majority_label] * test_data.label.shape[0]) \n",
    "# NOTE: the multiplication^^^ needs to be performed on a Python list, not on the array returned by pd.Series.mode().\n",
    "baseline_f1 = f1_score(baseline_pred, test_data.label)\n",
    "#baseline_mcc = matthews_corrcoef(test_data.label, baseline_pred)\n",
    "random_pred = np.random.randint(0, 2, size=test_data.label.shape[0])\n",
    "random_mcc = matthews_corrcoef(test_data.label, random_pred)\n",
    "\n",
    "## Get baseline accuracy by predicting the majority class.\n",
    "baseline_acc = test_data.label.value_counts().iloc[0]/test_data.label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(2* 10, 10))\n",
    "\n",
    "# Draw the lines.\n",
    "axes[0].plot(seed_performance, color=\"green\", label='bert_mcc_score')\n",
    "axes[0].plot(flip_hist, color=\"blue\", label='percent_acrobatic_sentences')\n",
    "#axes[0].axhline(random_mcc, color=\"red\", label='random_mcc', linestyle='-.')\n",
    "axes[0].axhline(baseline_acc, color=\"red\", label='baseline_accuracy', linestyle=':')\n",
    "\n",
    "\n",
    "axes[1].plot(seed_performance, color=\"green\", label='bert_mcc_score')\n",
    "axes[1].plot(1 - np.array(flip_hist), color=\"blue\", label='percent_unathletic_sentences')\n",
    "#axes[1].axhline(baseline_f1, color=\"red\", label='baseline_f1', linestyle='-.')\n",
    "axes[1].axhline(baseline_acc, color=\"red\", label='baseline_accuracy', linestyle=':')\n",
    "\n",
    "# Set axis bounds.\n",
    "#ax.set_xticks(range(len(RANDOM_SEEDS)))\n",
    "#ax.set_xticklabels(RANDOM_SEEDS)\n",
    "axes[0].set_ylim(0.0, 1.0)\n",
    "axes[1].set_ylim(0.5, 1.0)\n",
    "\n",
    "# Label everything.\n",
    "axes[0].set_title('Percentage of Acrobatic Sentences in the LI-Adger test set\\n' +\\\n",
    "                'as a function of the number of random seeds used to shuffle the training data')\n",
    "axes[1].set_title('Percentage of sentences in the LI-Adger test set whose predictions\\n' +\\\n",
    "                 'remain constant different orderings of the training data.')\n",
    "\n",
    "axes[0].set_xlabel(\"Number of different orderings of the CoLA training data\")\n",
    "axes[1].set_xlabel(\"Number of different orderings of the CoLA training data\")\n",
    "\n",
    "axes[0].set_ylabel(\"Percentage Acrobatic Senteces in the test set\")\n",
    "axes[1].set_ylabel(\"Percentage of Unathletic Sentences in the test set\")\n",
    "\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./figures/bert_stability_testing.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Draw the lines.\n",
    "ax.plot(seed_performance, color=\"green\", label='bert_f1')\n",
    "ax.plot(1 - np.array(flip_hist), color=\"blue\", label='consistent_sentences')\n",
    "ax.axhline(baseline_f1, color=\"red\", label='baseline_f1', linestyle='-.')\n",
    "ax.axhline(baseline_acc, color=\"red\", label='baseline_acc', linestyle=':')\n",
    "\n",
    "# Set axis bounds.\n",
    "#ax.set_xticks(range(len(RANDOM_SEEDS)))\n",
    "#ax.set_xticklabels(RANDOM_SEEDS)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "\n",
    "# Label everything.\n",
    "ax.set_title('Percentage of BERT predition changes across \\n as a function of the number of different orderings of the training data.')\n",
    "ax.set_xlabel(\"Number of different orderings of the CoLA training data\")\n",
    "ax.set_ylabel(\"Percentage of the test set\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export data for further analyses.\n",
    "np.savetxt(\"./output/bert_replicability_random_seeds.csv\", RANDOM_SEEDS, delimiter=\",\")\n",
    "test_data.to_csv(\"./output/bert_replicability_data.csv\")\n",
    "acrobatic_sentence.to_csv(\"./output/acrobatic_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data from language modelling.\n",
    "bert_data = pd.read_csv('./output/all_three_regularized/bert_acc_data.csv', index_col=0)\n",
    "\n",
    "# Select only the rows that have to do with BERT.\n",
    "bert_data = bert_data.loc[bert_data.measure.isin(['pseudoLogProb', 'acceptability'])]\n",
    "\n",
    "# Average the BERT acceptability predictions across random seeds.\n",
    "bert_data = bert_data.groupby(['id', 'measure', 'regularizedSentence', 'corpus', 'dataset'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the outliers in BERT's predictions.\n",
    "outliers = bert_data.loc[bert_data.probability < -60]\n",
    "\n",
    "# Get the flipping outliers.\n",
    "acro_outliers = outliers.loc[outliers.regularizedSentence.isin(acrobatic_sentences.sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acro_outliers = outliers.loc[outliers.regularizedSentence.isin(acrobatic_sentences.sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(acro_outliers, col='measure', row='dataset', height=5, aspect=1,\n",
    "                  sharey=False, sharex=False, margin_titles=True)\n",
    "g.map(sns.scatterplot, \"judgment\", \"probability\", color=\".3\", y_jitter=.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
